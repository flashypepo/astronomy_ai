{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af43584e",
   "metadata": {},
   "source": [
    "# Implementing a Simple Neural Network from Scratch using NumPy vs. Pytorch\n",
    "\n",
    "(as part of the Deep Learning & NLP course)\n",
    "\n",
    "**2025-0512 PP** use `conda` environment `Pytorch 2 (python 3.12)` (i.e. Apple Silicon, pytorch on GPU)\n",
    "\n",
    "**TODO** apply neural network on astronominal project: Habital/non-habital planets\n",
    "\n",
    "<br>\n",
    "Â© Thu Vu. All rights reserved.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bde03",
   "metadata": {},
   "source": [
    "ðŸ’¡ In this project, we will build a simple neural network to predict whether a flower will thrive or not based on the amount of sun and water it needs.\n",
    "\n",
    "Our neural network will have:\n",
    "- Two inputs \\(X_1\\) and \\(X_2\\)\n",
    "- A hidden layer with two neurons, \\(h_1\\) and \\(h_2\\)\n",
    "- An output layer that provides a single predicted output \\(y_pred\\)\n",
    "\n",
    "In addition, we will use a Sigmoid activation function and Scochastic Gradient Descent (SGD) for optimization.\n",
    "\n",
    "<img src=\"images/network.png\" alt=\"Network Image\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb0929",
   "metadata": {},
   "source": [
    "## Step 0: Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f6bd219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (2.1.2)\n",
      "Requirement already satisfied: pandas in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: torch in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from torch) (75.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thuvu/Documents/projects/python_projects/03_ML DL AI/myenv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the libraries we are going to use\n",
    "#PP I'll use Jupyter kernel 'Pytorch2 (Python 3.12)': ! pip install numpy pandas torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b025e3",
   "metadata": {},
   "source": [
    "## Step 1: Loading the Data\n",
    "We will use a small dataset called `flower_data_train.csv` that contains information about the amount of sun and water each flower received and whether it thrived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d527567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flower_id</th>\n",
       "      <th>sun</th>\n",
       "      <th>water</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.15</td>\n",
       "      <td>Did not thrive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>Thrived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Did not thrive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.55</td>\n",
       "      <td>Thrived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.60</td>\n",
       "      <td>Thrived</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flower_id   sun  water         outcome\n",
       "0          0  0.73   0.15  Did not thrive\n",
       "1          1  0.60   0.50         Thrived\n",
       "2          2  1.00   1.00  Did not thrive\n",
       "3          3  0.80   0.55         Thrived\n",
       "4          4  0.90   0.60         Thrived"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/flower_data_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da172650",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "We need to preprocess the data by encoding the outcome as a binary value (1 for 'Thrived' and 0 for 'Did not thrive'). We will also extract the features (sun and water) and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3757f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73 0.15]\n",
      " [0.6  0.5 ]\n",
      " [1.   1.  ]\n",
      " [0.8  0.55]\n",
      " [0.9  0.6 ]] [0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Encode the outcome\n",
    "data['outcome'] = data['outcome'].apply(lambda x: 1 if x == 'Thrived' else 0)\n",
    "\n",
    "# Extract features and labels\n",
    "X = data[['sun', 'water']].values\n",
    "y = data['outcome'].values\n",
    "print(X[:5], y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce5c3",
   "metadata": {},
   "source": [
    "## Step 3: Implementing the Neural Network from Scratch (VERY naive version)\n",
    "We will build a simple neural network with the following structure:\n",
    "- **Input Layer**: 2 neurons (sun and water)\n",
    "- **Hidden Layer**: 2 neurons\n",
    "- **Output Layer**: 1 neuron (sigmoid activation for binary classification)\n",
    "\n",
    "### Activation Function\n",
    "We will use the sigmoid function:\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f758ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    # Sigmoid function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5cd41",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "The cross-entropy loss is given by the formula:\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum \\left[ y_{\\text{true}} \\log(y_{\\text{pred}}) + (1 - y_{\\text{true}}) \\log(1 - y_{\\text{pred}}) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- N is the number of samples\n",
    "- y_true are the true labels (0 or 1)\n",
    "- y_pred are the predicted probabilities (between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f5771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels (0 or 1)\n",
    "    y_pred (numpy array): Predicted probabilities (between 0 and 1)\n",
    "\n",
    "    Returns:\n",
    "    float: The binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Clip y_pred to avoid log(0) and ensure numerical stability\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889e653",
   "metadata": {},
   "source": [
    "Now we will define our neural network and train it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8847b835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.6953976154\n",
      "Epoch 10 loss: 0.7025905718\n",
      "Epoch 20 loss: 0.7016244024\n",
      "Epoch 30 loss: 0.7007784134\n",
      "Epoch 40 loss: 0.7000184221\n",
      "Epoch 50 loss: 0.6993148162\n",
      "Epoch 60 loss: 0.6986393653\n",
      "Epoch 70 loss: 0.6979615931\n",
      "Epoch 80 loss: 0.6972462427\n",
      "Epoch 90 loss: 0.6964521230\n",
      "Epoch 100 loss: 0.6955322222\n",
      "Epoch 110 loss: 0.6944352553\n",
      "Epoch 120 loss: 0.6931093115\n",
      "Epoch 130 loss: 0.6915085455\n",
      "Epoch 140 loss: 0.6896034676\n",
      "Epoch 150 loss: 0.6873939615\n",
      "Epoch 160 loss: 0.6849216920\n",
      "Epoch 170 loss: 0.6822760909\n",
      "Epoch 180 loss: 0.6795878530\n",
      "Epoch 190 loss: 0.6770078078\n",
      "Epoch 200 loss: 0.6746761329\n",
      "Epoch 210 loss: 0.6726927937\n",
      "Epoch 220 loss: 0.6711004629\n",
      "Epoch 230 loss: 0.6698853000\n",
      "Epoch 240 loss: 0.6689924897\n",
      "Epoch 250 loss: 0.6683477396\n",
      "Epoch 260 loss: 0.6678760130\n",
      "Epoch 270 loss: 0.6675129713\n",
      "Epoch 280 loss: 0.6672090326\n",
      "Epoch 290 loss: 0.6669282331\n",
      "Epoch 300 loss: 0.6666442385\n",
      "Epoch 310 loss: 0.6663350538\n",
      "Epoch 320 loss: 0.6659772061\n",
      "Epoch 330 loss: 0.6655399417\n",
      "Epoch 340 loss: 0.6649804101\n",
      "Epoch 350 loss: 0.6642416099\n",
      "Epoch 360 loss: 0.6632548808\n",
      "Epoch 370 loss: 0.6619464085\n",
      "Epoch 380 loss: 0.6602434291\n",
      "Epoch 390 loss: 0.6580754200\n",
      "Epoch 400 loss: 0.6553710773\n",
      "Epoch 410 loss: 0.6520575844\n",
      "Epoch 420 loss: 0.6480677959\n",
      "Epoch 430 loss: 0.6433539830\n",
      "Epoch 440 loss: 0.6378990404\n",
      "Epoch 450 loss: 0.6317144942\n",
      "Epoch 460 loss: 0.6248236027\n",
      "Epoch 470 loss: 0.6172397037\n",
      "Epoch 480 loss: 0.6089513647\n",
      "Epoch 490 loss: 0.5999184630\n",
      "Epoch 500 loss: 0.5900786684\n",
      "Epoch 510 loss: 0.5793636960\n",
      "Epoch 520 loss: 0.5677237835\n",
      "Epoch 530 loss: 0.5551551275\n",
      "Epoch 540 loss: 0.5417206933\n",
      "Epoch 550 loss: 0.5275539046\n",
      "Epoch 560 loss: 0.5128403349\n",
      "Epoch 570 loss: 0.4977833717\n",
      "Epoch 580 loss: 0.4825696001\n",
      "Epoch 590 loss: 0.4673511415\n",
      "Epoch 600 loss: 0.4522530300\n",
      "Epoch 610 loss: 0.4373987303\n",
      "Epoch 620 loss: 0.4229353652\n",
      "Epoch 630 loss: 0.4090402636\n",
      "Epoch 640 loss: 0.3959029769\n",
      "Epoch 650 loss: 0.3836930082\n",
      "Epoch 660 loss: 0.3725307839\n",
      "Epoch 670 loss: 0.3624739752\n",
      "Epoch 680 loss: 0.3535203788\n",
      "Epoch 690 loss: 0.3456212098\n",
      "Epoch 700 loss: 0.3386975048\n",
      "Epoch 710 loss: 0.3326547876\n",
      "Epoch 720 loss: 0.3273940370\n",
      "Epoch 730 loss: 0.3228188772\n",
      "Epoch 740 loss: 0.3188397242\n",
      "Epoch 750 loss: 0.3153757786\n",
      "Epoch 760 loss: 0.3123556286\n",
      "Epoch 770 loss: 0.3097170263\n",
      "Epoch 780 loss: 0.3074062197\n",
      "Epoch 790 loss: 0.3053770838\n",
      "Epoch 800 loss: 0.3035901975\n",
      "Epoch 810 loss: 0.3020119483\n",
      "Epoch 820 loss: 0.3006137085\n",
      "Epoch 830 loss: 0.2993711001\n",
      "Epoch 840 loss: 0.2982633530\n",
      "Epoch 850 loss: 0.2972727524\n",
      "Epoch 860 loss: 0.2963841665\n",
      "Epoch 870 loss: 0.2955846470\n",
      "Epoch 880 loss: 0.2948630918\n",
      "Epoch 890 loss: 0.2942099608\n",
      "Epoch 900 loss: 0.2936170375\n",
      "Epoch 910 loss: 0.2930772285\n",
      "Epoch 920 loss: 0.2925843955\n",
      "Epoch 930 loss: 0.2921332137\n",
      "Epoch 940 loss: 0.2917190531\n",
      "Epoch 950 loss: 0.2913378784\n",
      "Epoch 960 loss: 0.2909861646\n",
      "Epoch 970 loss: 0.2906608254\n",
      "Epoch 980 loss: 0.2903591527\n",
      "Epoch 990 loss: 0.2900787658\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  Adapted from Victor Zhou's repo: https://github.com/vzhou842/neural-network-from-scratch\n",
    "  \n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (y_pred)\n",
    "\n",
    "  *** DISCLAIMER ***:\n",
    "  The code below is intended to be simple and educational. It's not optimal.\n",
    "  Real neural net code looks nothing like this. DO NOT use this code.\n",
    "  Instead, read/run it to understand how this specific network works.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Set the seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Weights\n",
    "    self.w1 = np.random.rand()\n",
    "    self.w2 = np.random.rand()\n",
    "    self.w3 = np.random.rand()\n",
    "    self.w4 = np.random.rand()\n",
    "    self.w5 = np.random.rand()\n",
    "    self.w6 = np.random.rand()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.rand()\n",
    "    self.b2 = np.random.rand()\n",
    "    self.b3 = np.random.rand()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, X, all_y_trues):\n",
    "    '''\n",
    "    - X is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.1   # how fast we want to train (adjusting weights and biases)\n",
    "    epochs = 1000      # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(X, all_y_trues):  # For each data pair of X and y like this: [0.73 0.15] 0\n",
    "\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_y_pred = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        y_pred = sigmoid(sum_y_pred)\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "\n",
    "        # Partial derivative of loss with respect to y_pred (see the note cell below for how we got this formula)\n",
    "        d_L_d_ypred = - (y_true / y_pred - (1 - y_true) / (1 - y_pred))\n",
    "\n",
    "        # Neuron y_pred\n",
    "        d_ypred_d_w5 = h1 * sigmoid_derivative(sum_y_pred)\n",
    "        d_ypred_d_w6 = h2 * sigmoid_derivative(sum_y_pred)\n",
    "        d_ypred_d_b3 = sigmoid_derivative(sum_y_pred)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * sigmoid_derivative(sum_y_pred)\n",
    "        d_ypred_d_h2 = self.w6 * sigmoid_derivative(sum_y_pred)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * sigmoid_derivative(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * sigmoid_derivative(sum_h1)\n",
    "        d_h1_d_b1 = sigmoid_derivative(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * sigmoid_derivative(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * sigmoid_derivative(sum_h2)\n",
    "        d_h2_d_b2 = sigmoid_derivative(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron y_pred\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      # Loss should decrease per epoch, otherwise something is wrong in our implementation\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, X)\n",
    "        loss = cross_entropy_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.10f\" % (epoch, loss))\n",
    "\n",
    "# Define all_y_trues\n",
    "all_y_trues = y\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(X, all_y_trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc332fdd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**ðŸ“˜ NOTE: Derivative of the cross-entropy loss (L) with respect to the prediction (y_pred)**\n",
    "\n",
    "Given the binary cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum \\left[ y_{\\text{true}} \\log(y_{\\text{pred}}) + (1 - y_{\\text{true}}) \\log(1 - y_{\\text{pred}}) \\right]\n",
    "$$\n",
    "\n",
    "For a single training example (drop the summation and $\\frac{1}{N}$):\n",
    "\n",
    "$$\n",
    "L = - \\left[ y_{\\text{true}} \\log(y_{\\text{pred}}) + (1 - y_{\\text{true}}) \\log(1 - y_{\\text{pred}}) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Take the derivative with respect to $y_{\\text{pred}}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_{\\text{pred}}} = - \\left( \\frac{y_{\\text{true}}}{y_{\\text{pred}}} - \\frac{1 - y_{\\text{true}}}{1 - y_{\\text{pred}}} \\right)\n",
    "$$\n",
    "\n",
    "Hence, in our code above, we wrote:\n",
    "\n",
    "d_L_d_ypred = - (y_true / y_pred - (1 - y_true) / (1 - y_pred))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f1928",
   "metadata": {},
   "source": [
    "## Step 4: Implementing the Neural Network using PyTorch\n",
    "Now, let's implement the same neural network using PyTorch, which will simplify the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ce0d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.73137366771698\n",
      "Epoch 10, Loss: 0.7282949090003967\n",
      "Epoch 20, Loss: 0.7254539132118225\n",
      "Epoch 30, Loss: 0.722833514213562\n",
      "Epoch 40, Loss: 0.7204175591468811\n",
      "Epoch 50, Loss: 0.7181909084320068\n",
      "Epoch 60, Loss: 0.716139554977417\n",
      "Epoch 70, Loss: 0.7142502665519714\n",
      "Epoch 80, Loss: 0.7125106453895569\n",
      "Epoch 90, Loss: 0.7109094262123108\n",
      "Epoch 100, Loss: 0.7094359993934631\n",
      "Epoch 110, Loss: 0.7080804705619812\n",
      "Epoch 120, Loss: 0.7068336009979248\n",
      "Epoch 130, Loss: 0.7056871056556702\n",
      "Epoch 140, Loss: 0.7046329975128174\n",
      "Epoch 150, Loss: 0.7036639451980591\n",
      "Epoch 160, Loss: 0.7027732729911804\n",
      "Epoch 170, Loss: 0.7019548416137695\n",
      "Epoch 180, Loss: 0.7012028694152832\n",
      "Epoch 190, Loss: 0.7005119323730469\n",
      "Epoch 200, Loss: 0.6998772621154785\n",
      "Epoch 210, Loss: 0.69929438829422\n",
      "Epoch 220, Loss: 0.6987590193748474\n",
      "Epoch 230, Loss: 0.6982674598693848\n",
      "Epoch 240, Loss: 0.6978158950805664\n",
      "Epoch 250, Loss: 0.6974014043807983\n",
      "Epoch 260, Loss: 0.6970208287239075\n",
      "Epoch 270, Loss: 0.6966714262962341\n",
      "Epoch 280, Loss: 0.6963507533073425\n",
      "Epoch 290, Loss: 0.6960562467575073\n",
      "Epoch 300, Loss: 0.6957859396934509\n",
      "Epoch 310, Loss: 0.6955378651618958\n",
      "Epoch 320, Loss: 0.695310115814209\n",
      "Epoch 330, Loss: 0.6951010227203369\n",
      "Epoch 340, Loss: 0.6949090957641602\n",
      "Epoch 350, Loss: 0.6947330236434937\n",
      "Epoch 360, Loss: 0.6945713758468628\n",
      "Epoch 370, Loss: 0.6944230198860168\n",
      "Epoch 380, Loss: 0.6942868232727051\n",
      "Epoch 390, Loss: 0.6941617727279663\n",
      "Epoch 400, Loss: 0.6940470337867737\n",
      "Epoch 410, Loss: 0.6939417123794556\n",
      "Epoch 420, Loss: 0.6938450336456299\n",
      "Epoch 430, Loss: 0.6937562227249146\n",
      "Epoch 440, Loss: 0.6936747431755066\n",
      "Epoch 450, Loss: 0.6935998797416687\n",
      "Epoch 460, Loss: 0.6935311555862427\n",
      "Epoch 470, Loss: 0.6934680342674255\n",
      "Epoch 480, Loss: 0.6934100985527039\n",
      "Epoch 490, Loss: 0.693356990814209\n",
      "Epoch 500, Loss: 0.6933080554008484\n",
      "Epoch 510, Loss: 0.693263053894043\n",
      "Epoch 520, Loss: 0.6932218074798584\n",
      "Epoch 530, Loss: 0.6931838989257812\n",
      "Epoch 540, Loss: 0.6931489706039429\n",
      "Epoch 550, Loss: 0.6931169033050537\n",
      "Epoch 560, Loss: 0.6930875778198242\n",
      "Epoch 570, Loss: 0.6930603384971619\n",
      "Epoch 580, Loss: 0.6930354237556458\n",
      "Epoch 590, Loss: 0.693012535572052\n",
      "Epoch 600, Loss: 0.6929914355278015\n",
      "Epoch 610, Loss: 0.69297194480896\n",
      "Epoch 620, Loss: 0.6929540634155273\n",
      "Epoch 630, Loss: 0.6929375529289246\n",
      "Epoch 640, Loss: 0.6929222941398621\n",
      "Epoch 650, Loss: 0.6929082870483398\n",
      "Epoch 660, Loss: 0.6928954124450684\n",
      "Epoch 670, Loss: 0.6928833723068237\n",
      "Epoch 680, Loss: 0.6928724646568298\n",
      "Epoch 690, Loss: 0.6928621530532837\n",
      "Epoch 700, Loss: 0.692852795124054\n",
      "Epoch 710, Loss: 0.6928440928459167\n",
      "Epoch 720, Loss: 0.6928359866142273\n",
      "Epoch 730, Loss: 0.6928284764289856\n",
      "Epoch 740, Loss: 0.6928215622901917\n",
      "Epoch 750, Loss: 0.6928150653839111\n",
      "Epoch 760, Loss: 0.6928091049194336\n",
      "Epoch 770, Loss: 0.6928035020828247\n",
      "Epoch 780, Loss: 0.6927983164787292\n",
      "Epoch 790, Loss: 0.6927934885025024\n",
      "Epoch 800, Loss: 0.6927889585494995\n",
      "Epoch 810, Loss: 0.6927847862243652\n",
      "Epoch 820, Loss: 0.6927807927131653\n",
      "Epoch 830, Loss: 0.6927770972251892\n",
      "Epoch 840, Loss: 0.6927735805511475\n",
      "Epoch 850, Loss: 0.6927703619003296\n",
      "Epoch 860, Loss: 0.6927672624588013\n",
      "Epoch 870, Loss: 0.6927643418312073\n",
      "Epoch 880, Loss: 0.6927616596221924\n",
      "Epoch 890, Loss: 0.6927590370178223\n",
      "Epoch 900, Loss: 0.6927565932273865\n",
      "Epoch 910, Loss: 0.6927543878555298\n",
      "Epoch 920, Loss: 0.6927520036697388\n",
      "Epoch 930, Loss: 0.6927499771118164\n",
      "Epoch 940, Loss: 0.692747950553894\n",
      "Epoch 950, Loss: 0.6927459836006165\n",
      "Epoch 960, Loss: 0.6927441358566284\n",
      "Epoch 970, Loss: 0.6927423477172852\n",
      "Epoch 980, Loss: 0.6927406787872314\n",
      "Epoch 990, Loss: 0.6927390098571777\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Define the neural network class\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 2)\n",
    "        self.output = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer with learning rate = 0.01\n",
    "model = SimpleNeuralNetwork()\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f135f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2 (python 3.12)",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
